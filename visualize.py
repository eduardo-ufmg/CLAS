import argparse
import pathlib
import subprocess
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Any, Tuple

NUM_DIMENSIONS = 2
NUM_CLASSES = 2

def generate_synthetic_data(num_vertices: int, spread_factor: float, output_file: str, random_seed: int = None, return_centers: bool = False) -> Any:
    """
    Generate synthetic 2D training data with a specified number of vertices and spread factor.
    Each row contains the feature values and a class label. Optionally returns the cluster centers.
    
    Args:
      num_vertices: Total number of vertices to generate.
      spread_factor: Standard deviation for the clusters.
      output_file: File path to save the generated data.
      random_seed: Optional seed for reproducibility.
      return_centers: If True, also return the list of cluster centers.
      
    Returns:
      A numpy array of the generated data, and if return_centers is True, a list of centers.
    """
    if random_seed is not None:
        np.random.seed(random_seed)
    
    base = num_vertices // NUM_CLASSES
    counts = [base] * NUM_CLASSES
    for i in range(num_vertices - base * NUM_CLASSES):
        counts[i] += 1

    data_rows = []
    centers = []
    # Use an explicit mapping for class labels.
    class_labels = [-1, 1]
    
    for label, count in zip(class_labels, counts):
        center = np.random.uniform(0, 10, NUM_DIMENSIONS)
        centers.append(center)
        points = np.random.normal(loc=center, scale=spread_factor, size=(count, NUM_DIMENSIONS))
        for point in points:
            data_rows.append(list(point) + [label])
    
    data_array = np.array(data_rows)
    pathlib.Path(output_file).parent.mkdir(parents=True, exist_ok=True)
    fmt = ("%.6f," * NUM_DIMENSIONS + "%d")
    np.savetxt(output_file, data_array, delimiter=",", fmt=fmt)
    print(f"Synthetic training data saved to {output_file}")
    
    if return_centers:
        return data_array, centers
    else:
        return data_array

def generate_to_classify_data(centers: List[np.ndarray], total_num_vertices: int, spread_factor: float, output_file: str, random_seed: int = None) -> np.ndarray:
    """
    Generate synthetic to-classify 2D data. The points are generated around the given cluster centers.
    Each row contains only the feature coordinates (no class label).
    
    Args:
      centers: List of cluster centers (as generated by the training data).
      total_num_vertices: Total number of to-classify vertices to generate (should be half of training vertices).
      spread_factor: Spread factor for the to-classify data.
      output_file: File path to save the generated to-classify data.
      random_seed: Optional seed for reproducibility.
      
    Returns:
      A numpy array of the generated to-classify data.
    """
    if random_seed is not None:
        # Offset the seed so that the to-classify data is different from the training data
        np.random.seed(random_seed + 1)
    
    base = total_num_vertices // NUM_CLASSES
    counts = [base] * NUM_CLASSES
    for i in range(total_num_vertices - base * NUM_CLASSES):
        counts[i] += 1

    data_rows = []
    for center, count in zip(centers, counts):
        points = np.random.normal(loc=center, scale=spread_factor, size=(count, NUM_DIMENSIONS))
        for point in points:
            data_rows.append(list(point))
    
    data_array = np.array(data_rows)
    pathlib.Path(output_file).parent.mkdir(parents=True, exist_ok=True)
    fmt = ("%.6f," * NUM_DIMENSIONS).rstrip(',')
    np.savetxt(output_file, data_array, delimiter=",", fmt=fmt)
    print(f"To-classify data saved to {output_file}")
    return data_array

def plot_synthetic_data(ax: Any, data: np.ndarray, title: str = "Synthetic Data") -> None:
    """
    Plot synthetic training data on the given matplotlib axis.
    
    Args:
      ax: Matplotlib axis to plot on.
      data: Data array containing features and class labels.
      title: Plot title.
    """
    unique_classes = np.unique(data[:, -1])
    cmap = plt.get_cmap("viridis", len(unique_classes))
    for label in unique_classes:
        mask = data[:, -1] == label
        # Ensure the colormap index is an integer
        color_index = int(label) if isinstance(label, (int, np.integer, float)) else 0
        ax.scatter(data[mask, 0], data[mask, 1],
                   label=f"Class {int(label)}", color=cmap(color_index))
    ax.set_xlabel("Feature 1")
    ax.set_ylabel("Feature 2")
    ax.set_title(title)
    ax.legend()

def parse_graph_file(filename: str) -> Dict[int, Dict[str, Any]]:
    """
    Parse a graph file where each non-empty line has the format:
      vertex_id | features | cluster | adjacent
    The adjacent field should be a comma-separated list of "id-flag" pairs.
    
    Args:
      filename: Path to the graph file.
    
    Returns:
      A dictionary mapping vertex id to its data (coordinates, cluster, adjacent vertices).
    """
    graph: Dict[int, Dict[str, Any]] = {}
    with open(filename, "r") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            parts = line.split("|")
            if len(parts) < 4:
                continue
            try:
                v_id = int(parts[0].replace(",", "").strip())
                features = [float(x.strip()) for x in parts[1].split(",") if x.strip()]
                cluster = float(parts[2].replace(",", "").strip())
                adjacent = []
                for pair in parts[3].split(','):
                    if pair.strip():
                        adj_parts = pair.split('-')
                        if len(adj_parts) != 2:
                            continue
                        adj_id = int(adj_parts[0].strip())
                        is_se = adj_parts[1].strip() == "1"
                        adjacent.append((adj_id, is_se))
                graph[v_id] = {"coords": np.array(features), "cluster": cluster, "adjacent": adjacent}
            except Exception as e:
                print(f"Error parsing line: {line}. Error: {e}")
                continue
    return graph

def plot_graph(ax: Any, graph: Dict[int, Dict[str, Any]], title: str = "Graph", draw_edges=True) -> None:
    """
    Plot a graph on the provided axis. Each vertex is plotted as a scatter point, and edges
    are drawn between adjacent vertices if `draw_edges` is `True`
    
    Args:
      ax: Matplotlib axis to plot on.
      graph: Dictionary representing the graph.
      title: Plot title.
    """
    clusters = {node["cluster"] for node in graph.values()}
    cmap = plt.get_cmap("viridis", len(clusters))
    coords = {v_id: data["coords"] for v_id, data in graph.items()}

    for v_id, data in graph.items():
        cluster_int = int(data["cluster"]) if isinstance(data["cluster"], (int, np.integer, float)) else 0
        ax.scatter(data["coords"][0], data["coords"][1],
                   color=cmap(cluster_int), s=50)
    
    drawn = set()
    if draw_edges:
      for v_id, data in graph.items():
          for adj, is_se in data.get("adjacent", []):
              if adj not in coords:
                  continue
              if (adj, v_id) in drawn:
                  continue
              p1 = coords[v_id]
              p2 = coords[adj]
              line_color = "r-" if is_se else "k-"
              ax.plot([p1[0], p2[0]], [p1[1], p2[1]], line_color, lw=1)
              drawn.add((v_id, adj))
    ax.set_xlabel("Feature 1")
    ax.set_ylabel("Feature 2")
    ax.set_title(title)

def parse_hyperplanes_file(filename: str) -> np.ndarray:
    """
    Parse the hyperplanes output file and extract the midpoint coordinates for each support edge.
    
    Expected format per line:
      id, |, v0, v1, |, diff_coord0, diff_coord1, ..., |, midpoint_coord0, midpoint_coord1, ..., |, bias
    
    Args:
      filename: Path to the hyperplanes output file.
      
    Returns:
      A numpy array of midpoints (each row is a midpoint coordinate).
    """
    midpoints = []
    with open(filename, "r") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            parts = line.split("|")
            if len(parts) < 4:
                continue
            # The midpoint coordinates are in the fourth part (index 3)
            midpoint_str = parts[3].strip()
            # Split by comma and filter out empty strings
            midpoint_vals = [float(x.strip()) for x in midpoint_str.split(",") if x.strip()]
            if len(midpoint_vals) == NUM_DIMENSIONS:
                midpoints.append(midpoint_vals)
    if midpoints:
        midpoints = np.array(midpoints)
    else:
        midpoints = np.empty((0, NUM_DIMENSIONS))
    return midpoints

def parse_chip_clas_file(filename: str) -> Dict[int, int]:
    """
    Parse the chip-clas output file which has lines of the format:
      id, class
    
    Returns:
      A dictionary mapping the vertex id (int) to the classified class (int).
    """
    classifications = {}
    with open(filename, "r") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                parts = line.split(",")
                v_id = int(parts[0].strip())
                v_class = int(parts[1].strip())
                classifications[v_id] = v_class
            except Exception as e:
                print(f"Error parsing chip-clas line: {line}. Error: {e}")
    return classifications

def compute_nearest_neighbor_path(points: np.ndarray) -> List[int]:
    """
    Given an array of points (n x NUM_DIMENSIONS), compute a nearest neighbor tour starting 
    from the first point, then refine it using a two-opt algorithm.
    Returns a list of indices representing the order.
    """
    n = len(points)
    if n == 0:
        return []
    
    # Greedy nearest neighbor route
    visited = set([0])
    route = [0]
    current = 0
    while len(visited) < n:
        dists = np.linalg.norm(points - points[current], axis=1)
        for idx in visited:
            dists[idx] = np.inf
        next_idx = int(np.argmin(dists))
        route.append(next_idx)
        visited.add(next_idx)
        current = next_idx

    # Refine the route using 2-opt improvement
    route = two_opt(route, points)
    return route

def route_distance(route: List[int], points: np.ndarray) -> float:
    """Compute total distance of the tour given a route (list of indices)."""
    distance = 0.0
    for i in range(len(route) - 1):
        distance += np.linalg.norm(points[route[i]] - points[route[i+1]])
    return distance

def two_opt(route: List[int], points: np.ndarray) -> List[int]:
    """
    Apply the two-opt algorithm to improve the tour by swapping segments.
    """
    best = route
    improved = True
    while improved:
        improved = False
        for i in range(1, len(best) - 2):
            for j in range(i + 1, len(best)):
                if j - i == 1:  # skip adjacent indices
                    continue
                new_route = best[:i] + best[i:j][::-1] + best[j:]
                if route_distance(new_route, points) < route_distance(best, points):
                    best = new_route
                    improved = True
        route = best
    return best

# --- Extending the polyline to the plot boundaries ---
def extend_point_to_bounds(point: np.ndarray, direction: np.ndarray, xlim: tuple, ylim: tuple) -> np.ndarray:
    """
    Extend a point along a given direction until it hits the plot boundaries.
    """
    if np.allclose(direction, 0):
        return point.copy()
    
    t_candidates = []
    # Calculate t for vertical boundaries (x-direction)
    if direction[0] != 0:
        if direction[0] > 0:
            t_x = (xlim[1] - point[0]) / direction[0]
        else:
            t_x = (xlim[0] - point[0]) / direction[0]
        t_candidates.append(t_x)
    
    # Calculate t for horizontal boundaries (y-direction)
    if direction[1] != 0:
        if direction[1] > 0:
            t_y = (ylim[1] - point[1]) / direction[1]
        else:
            t_y = (ylim[0] - point[1]) / direction[1]
        t_candidates.append(t_y)
    
    # Choose the smallest positive t
    t = min(t for t in t_candidates if t > 0)
    return point + t * direction

def extend_polyline_to_bounds(polyline: np.ndarray, ax) -> np.ndarray:
    """
    Given a polyline (n x 2), extend its start and end in the directions
    of the first and last segments so that the line touches the plot boundaries.
    """
    xlim = ax.get_xlim()
    ylim = ax.get_ylim()
    
    # Extend start
    start = polyline[0]
    second = polyline[1]
    start_direction = start - second  # reverse direction for extension
    extended_start = extend_point_to_bounds(start, start_direction, xlim, ylim)
    
    # Extend end
    end = polyline[-1]
    penultimate = polyline[-2]
    end_direction = end - penultimate  # forward direction
    extended_end = extend_point_to_bounds(end, end_direction, xlim, ylim)
    
    # Combine the extended endpoints with the original polyline
    extended_polyline = np.vstack([extended_start, polyline, extended_end])
    return extended_polyline

def run_command(cmd_input_file: List[str], cwd: str) -> None:
    """
    Run a subprocess command in the specified working directory.
    
    Args:
      cmd_input_file: List of command arguments.
      cwd: Working directory in which to execute the command.
    
    Raises:
      subprocess.CalledProcessError if the command fails.
    """
    print("Running command:", " ".join(cmd_input_file))
    try:
        subprocess.run(cmd_input_file, check=True, cwd=cwd)
        print("Command finished successfully.")
    except subprocess.CalledProcessError as e:
        print(f"Command failed: {e}")
        raise

def main():
    parser = argparse.ArgumentParser(
        description="Generate synthetic training and to-classify data, run Gabriel Graph and Low Degree Filter, then run hyperplanes and chip-clas, and plot results."
    )
    parser.add_argument("--num_vertices", type=int, help="Total number of training vertices", default=100)
    parser.add_argument("--spread", type=float, help="Spread factor for the training clusters", default=0.5)
    parser.add_argument("--to_classify_spread", type=float, help="Spread factor for the to-classify clusters", default=0.5)
    parser.add_argument("--seed", type=int, help="Random seed for reproducibility", default=None)
    parser.add_argument("--threshold_factor", type=float, help="Threshold factor for the filter", default=1.0)
    args = parser.parse_args()
    
    # Define file paths (relative to project root)
    synthetic_data_file_from_root = "data/synthetic/synthetic.csv"
    vertices_to_classify_from_root = "data/synthetic/to-classify.csv"
    
    gabriel_graph_exe_from_root = "cpp/GabrielGraph/gabrielGraph"
    gabriel_output_from_root = "cpp/GabrielGraph/output/synthetic.csv"
    gabriel_filtered_output_from_root = "cpp/GabrielGraph/output/synthetic-filtered.csv"
    filter_exe_from_root = "cpp/Filter/filter"
    filter_output_from_root = "cpp/Filter/output/synthetic-filtered.csv"
    
    # Hyperplanes parameters
    hyperplanes_from_root = "cpp/classifiers/Common/hyperplanes"
    hyperplanes_output_from_root = "cpp/classifiers/Common/output/synthetic-filtered.csv"
    path_from_hyperplanes_to_root = "../../../"
    hyperplanes_cwd = "cpp/classifiers/Common"
    hyperplanes_exe = "./hyperplanes"
    
    # Chip-clas parameters
    chip_clas_from_root = "cpp/classifiers/chip-clas/chip-clas"
    chip_clas_output_from_root = "cpp/classifiers/chip-clas/output/synthetic-classified.csv"
    path_from_chip_clas_to_root = "../../../"
    chip_clas_cwd = "cpp/classifiers/chip-clas"
    chip_clas_exe = "./chip-clas"
    
    # Other path parameters (from executables to project root)
    path_from_gabriel_to_root = "../../"
    path_from_filter_to_root = "../../"
    
    # Generate synthetic training data and capture the centers
    training_data, centers = generate_synthetic_data(
        args.num_vertices, args.spread, synthetic_data_file_from_root, args.seed, return_centers=True
    )
    
    # Generate synthetic to-classify data (half as many vertices) using the same centers
    num_to_classify = args.num_vertices // 2
    to_classify_data = generate_to_classify_data(
        centers, num_to_classify, args.to_classify_spread, vertices_to_classify_from_root, args.seed
    )
    
    # Create the figure with two subplots
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    # Left plot: original training dataset
    plot_synthetic_data(axes[0], training_data, title="Training Data")
    
    # Run Gabriel Graph executable on training data
    gabriel_exe = "./gabrielGraph"
    gabriel_input_file = path_from_gabriel_to_root + synthetic_data_file_from_root
    run_command([gabriel_exe, gabriel_input_file], "cpp/GabrielGraph")
    
    # Run Low Degree Filter executable
    filter_exe = "./filter"
    filter_input_file = path_from_filter_to_root + gabriel_output_from_root
    run_command([filter_exe, filter_input_file, str(args.threshold_factor)], "cpp/Filter")
    
    # Recompute Gabriel Graph on the filtered data
    run_command([gabriel_exe, path_from_gabriel_to_root + filter_output_from_root], "cpp/GabrielGraph")
    
    # Parse and plot the filtered graph on the right subplot
    if not pathlib.Path(gabriel_filtered_output_from_root).exists():
        print(f"Error: Expected Low Degree Filter output not found at {gabriel_filtered_output_from_root}")
    else:
        filtered_graph = parse_graph_file(gabriel_filtered_output_from_root)
        plot_graph(axes[1], filtered_graph, title="Filtered Graph with Classification & Hyperplane", draw_edges=False)
    
    # Run hyperplanes executable on the filtered Gabriel Graph
    run_command([hyperplanes_exe, path_from_hyperplanes_to_root + gabriel_filtered_output_from_root], hyperplanes_cwd)
    
    # Run chip-clas executable on the to-classify data
    run_command(
        [chip_clas_exe,
         path_from_chip_clas_to_root + gabriel_filtered_output_from_root,
         path_from_chip_clas_to_root + hyperplanes_output_from_root,
         path_from_chip_clas_to_root + vertices_to_classify_from_root],
        chip_clas_cwd
    )
    
    # Parse chip-clas output and merge classification with to-classify data
    classifications = parse_chip_clas_file(chip_clas_output_from_root)

    to_classify_classes = np.array([classifications.get(i + (1 - num_to_classify), 0) for i in range(len(to_classify_data))])
    
    # Plot the classified to-classify vertices on the right subplot
    unique_classes = np.unique(to_classify_classes)
    cmap = plt.get_cmap("plasma", len(unique_classes))
    for idx, cl in enumerate(unique_classes):
        mask = to_classify_classes == cl
        axes[1].scatter(
            to_classify_data[mask, 0],
            to_classify_data[mask, 1],
            marker="D", edgecolor="k", facecolor=cmap(idx),
            s=80, label=f"Classified {cl}"
        )
    
    axes[1].legend()
    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    main()
